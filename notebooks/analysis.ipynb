{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Setting credentials and reading CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    " \"fs.azure.account.key.<storage_account_name>.blob.core.windows.net\",\n",
    " \"<your_account_key>\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Load CSV files from Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"wasbs://<your_container_name>@<your_storage_account_name>.blob.core.windows.net/<your_csv_file_path>\", header=True, inferSchema=True)\n",
    "print(f\"Number of rows in the DataFrame: {df.count()}\")\n",
    "print(f\"Number of partitions: {df.rdd.getNumPartitions()}\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Filter rows based on conditions to do some analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 1: Filter by prices greater than 2000 USD\n",
    "filtered_df_1 = df.filter(df[\"`Current Price`\"] > 2000)\n",
    "print(\"Filtered by Current Price > 2000:\")\n",
    "display(filtered_df_1)\n",
    "\n",
    "# Filter 2: Filter by prices less than 101 USD\n",
    "filtered_df_2 = df.filter(df[\"`Current Price`\"] < 101)\n",
    "print(\"Filtered by Current Price < 101:\")\n",
    "display(filtered_df_2)\n",
    "\n",
    "# Filter 3: Filter by positive 24h Change (if the column exists)\n",
    "filtered_df_3 = df.filter(df[\"`24h Change`\"] > 0)\n",
    "print(\"Filtered by 24h Change > 0:\")\n",
    "display(filtered_df_3)\n",
    "\n",
    "# Filter 4: Filter by negative 24h Change (if the column exists)\n",
    "filtered_df_4 = df.filter(df[\"`24h Change`\"] < 0)\n",
    "print(\"Filtered by 24h Change < 0:\")\n",
    "display(filtered_df_4)\n",
    "\n",
    "# Filter 5: Filter by coins containing \"Bitcoin\" in the Name\n",
    "filtered_df_5 = df.filter(df[\"Name\"].like(\"%Bitcoin%\"))\n",
    "print(\"Filtered by Name containing 'Bitcoin':\")\n",
    "display(filtered_df_5)\n",
    "\n",
    "# Filter 6: Filter by coins specifically named 'Ethereum'\n",
    "filtered_df_6 = df.filter(df[\"Name\"] == \"Ethereum\")\n",
    "print(\"Filtered by Name 'Ethereum':\")\n",
    "display(filtered_df_6)\n",
    "\n",
    "# Filter 7: Filter by coins specifically named 'Litecoin'\n",
    "filtered_df_7 = df.filter(df[\"Name\"] == \"Litecoin\")\n",
    "print(\"Filtered by Name 'Litecoin':\")\n",
    "display(filtered_df_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Save the results as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max, avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Get the most recent 'Creation Date' for each 'Name'\n",
    "latest_creation_date_df = df.withColumn(\"Creation Date\", col(\"`Creation Date`\").cast(\"timestamp\"))\n",
    "latest_creation_date_df = latest_creation_date_df.withColumn(\n",
    "    \"max_creation_date\", max(\"Creation Date\").over(Window.partitionBy(\"Name\"))\n",
    ")\n",
    "\n",
    "# Filter rows where 'Creation Date' is the most recent\n",
    "filtered_df = latest_creation_date_df.filter(col(\"Creation Date\") == col(\"max_creation_date\")).drop(\"max_creation_date\")\n",
    "\n",
    "# Group by 'Name' and calculate the average of '24h Change', and keep the most recent 'Current Price'\n",
    "agg_df = filtered_df.groupBy(\"Name\").agg(\n",
    "    avg(\"`24h Change`\").alias(\"AVG price change\"),  # Temporary alias\n",
    "    max(\"`Current Price`\")  # Keep the most recent 'Current Price'\n",
    ")\n",
    "\n",
    "# Rename the 'AVG price change' column permanently\n",
    "result_df = agg_df.withColumnRenamed(\"AVG price change\", \"Average Price Change\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "display(result_df)\n",
    "\n",
    "# Write the result to a CSV file in /dbfs/tmp/result_df with semicolon as the delimiter\n",
    "output_path = \"/dbfs/tmp/result_df\"\n",
    "result_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").option(\"delimiter\", \";\").csv(output_path)\n",
    "\n",
    "# Find the generated CSV file in the directory\n",
    "csv_files = dbutils.fs.ls(output_path)\n",
    "csv_file_path = \"\"\n",
    "for file_info in csv_files:\n",
    "    if file_info.name.endswith(\".csv\"):\n",
    "        csv_file_path = file_info.path\n",
    "        break\n",
    "\n",
    "if csv_file_path:\n",
    "    # Move the file to the /FileStore directory\n",
    "    dbutils.fs.mv(csv_file_path, \"dbfs:/FileStore/result_df.csv\")\n",
    "    # Read the contents of the CSV file and print it\n",
    "    file_content = dbutils.fs.head(\"dbfs:/FileStore/result_df.csv\")\n",
    "    result_df.show()\n",
    "    print(\"File content:\\n\")\n",
    "    print(file_content)\n",
    "    print(f\"\\nNumber of rows in result_df: {result_df.count()}\")\n",
    "    print(\"The prices are in USD.\")\n",
    "    print(\"The 'Average Price Change' is the result of averaging all the daily price changes.\")\n",
    "else:\n",
    "    print(\"No CSV file was generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: Check things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/dbfs/' section:\n",
    "\n",
    "#dbutils.fs.ls(\"dbfs/tmp/\") # Checks the 'tmp' directory to see if the 'result_df' directory was successfully created\n",
    "\n",
    "#dbutils.fs.ls(\"/dbfs/tmp/result_df/\") # Checks if the files on 'result_df' were successfully created (The one you want is the 'part-00000-tid-<id>.csv')\n",
    "\n",
    "#dbutils.fs.head(\"/dbfs/tmp/result_df/part-00000-tid-???.csv\") # Check the content of the file, you need to fill the id\n",
    "\n",
    "#dbutils.fs.rm(\"/dbfs/tmp/result_df/\", recurse=True) # Deletes the 'result_df' directory (don't worry because the code above creates it again)\n",
    "\n",
    "\n",
    "# 'dbfs:/' section:   <-- This is the one you need to use\n",
    "\n",
    "#dbutils.fs.rm(\"dbfs:/FileStore/result_df.csv\", recurse=True) # Deletes the 'result_df.csv' file\n",
    "\n",
    "#dbutils.fs.ls(\"dbfs:/FileStore/\") # Checks the 'FileStore' directory to see if the 'result_df.csv' file was successfully created\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
